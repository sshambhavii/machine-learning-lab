import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from scipy.stats import zscore
import matplotlib.pyplot as plt
from statsmodels.graphics.regressionplots import influence_plot

df=pd.read_csv('cust2.csv')
df2=pd.read_csv('cust2.csv')

#Correlation signifies fashion_prod as input feature
print(df.corr())

X=sm.add_constant(df['fashion_prod'])
X.head()
Y=df['Savings']
train_X, test_X, train_y, test_y = train_test_split(X,Y,train_size = 0.8, random_state = 100)
savings_lm=sm.OLS(train_y,train_X).fit()
print(savings_lm.summary2())
print(savings_lm.params)

Output:
                 Results: Ordinary least squares
=================================================================
Model:              OLS              Adj. R-squared:     0.563   
Dependent Variable: Savings          AIC:                185.2187
Date:               2023-09-13 15:48 BIC:                187.8831
No. Observations:   28               Log-Likelihood:     -90.609 
Df Model:           1                F-statistic:        35.74   
Df Residuals:       26               Prob (F-statistic): 2.60e-06
R-squared:          0.579            Scale:              40.786  
-----------------------------------------------------------------
                  Coef.  Std.Err.    t    P>|t|   [0.025   0.975]
-----------------------------------------------------------------
const            -8.7200   1.8283 -4.7695 0.0001 -12.4781 -4.9619
fashion_prod      1.0579   0.1769  5.9787 0.0000   0.6942  1.4216
-----------------------------------------------------------------
Omnibus:              13.766       Durbin-Watson:          2.509 
Prob(Omnibus):        0.001        Jarque-Bera (JB):       14.348
Skew:                 -1.294       Prob(JB):               0.001 
Kurtosis:             5.366        Condition No.:          16    
=================================================================

const          -8.720007
fashion_prod    1.057888

predicted=savings_lm.predict(X)

r_square=r2_score(df['Savings'],predicted)
print(r_square)
Output: 0.5813881216953113

print(mean_squared_error(df['Savings'],predicted))
Output: 32.85957998330374

#Outlier using Z-score
df['zscore_savings']=zscore(predicted)
df[ (df.zscore_savings > 3.0) | (df.zscore_savings < -3.0) ]
Output: None

#Cook's Distance
savings_influence = savings_lm.get_influence()
(c, p) = savings_influence.cooks_distance
plt.stem(np.arange( len( train_X) ),np.round( c, 3 ),markerfmt=",")
plt.title("Cooks distance for all observations")
plt.xlabel("Row index")
plt.ylabel("Cooks Distance")
Output: None

#Leverage Values
fig, ax = plt.subplots( figsize=(8,6) )
influence_plot( savings_lm, ax = ax ) 
plt.title("Leverage Value Vs Residuals") 
plt.show()
Output: 12th Objservation>3(k+1)/n

Leverage value gives one outlier while z-score & cook's distance give no outlier. Therefor we create a new df by removing the 12th observation and compare the r2-squared value.
value_to_remove = 22.5
df2= df2[df2['fashion_prod'] != value_to_remove]
X_new=sm.add_constant(df2['fashion_prod'])
Y_new=df2['Savings']
train_X_new, test_X_new, train_y_new, test_y_new = train_test_split(X_new,Y_new,train_size = 0.8, random_state = 100)
savings_lm_new=sm.OLS(train_y_new,train_X_new).fit()
print(savings_lm.params)
print(savings_lm_new.params)
predicted=savings_lm_new.predict(X_new)
Output:
const          -8.720007
fashion_prod    1.057888
dtype: float64
const          -8.563966
fashion_prod    0.991719
dtype: float64
print(savings_lm.summary2())
print(savings_lm_new.summary2())

Output:
                 Results: Ordinary least squares
=================================================================
Model:              OLS              Adj. R-squared:     0.563   
Dependent Variable: Savings          AIC:                185.2187
Date:               2023-09-13 15:52 BIC:                187.8831
No. Observations:   28               Log-Likelihood:     -90.609 
Df Model:           1                F-statistic:        35.74   
Df Residuals:       26               Prob (F-statistic): 2.60e-06
R-squared:          0.579            Scale:              40.786  
-----------------------------------------------------------------
                  Coef.  Std.Err.    t    P>|t|   [0.025   0.975]
-----------------------------------------------------------------
const            -8.7200   1.8283 -4.7695 0.0001 -12.4781 -4.9619
fashion_prod      1.0579   0.1769  5.9787 0.0000   0.6942  1.4216
-----------------------------------------------------------------
Omnibus:              13.766       Durbin-Watson:          2.509 
Prob(Omnibus):        0.001        Jarque-Bera (JB):       14.348
Skew:                 -1.294       Prob(JB):               0.001 
Kurtosis:             5.366        Condition No.:          16    
=================================================================

                 Results: Ordinary least squares
=================================================================
Model:              OLS              Adj. R-squared:     0.482   
Dependent Variable: Savings          AIC:                179.4540
Date:               2023-09-13 15:52 BIC:                182.0457
No. Observations:   27               Log-Likelihood:     -87.727 
Df Model:           1                F-statistic:        25.20   
Df Residuals:       25               Prob (F-statistic): 3.54e-05
R-squared:          0.502            Scale:              41.988  
-----------------------------------------------------------------
                  Coef.  Std.Err.    t    P>|t|   [0.025   0.975]
-----------------------------------------------------------------
const            -8.5640   1.8542 -4.6187 0.0001 -12.3827 -4.7452
fashion_prod      0.9917   0.1976  5.0195 0.0000   0.5848  1.3986
-----------------------------------------------------------------
Omnibus:              13.069       Durbin-Watson:          2.570 
Prob(Omnibus):        0.001        Jarque-Bera (JB):       13.353
Skew:                 -1.233       Prob(JB):               0.001 
Kurtosis:             5.405        Condition No.:          14    
=================================================================


We observe that the r-squared value decreases.